{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.vector import Document, Model, TFIDF, LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999\n"
     ]
    }
   ],
   "source": [
    "# I have a file of 2999 presidential documents,\n",
    "# scraped from the American Presidency Project:\n",
    "# http://www.presidency.ucsb.edu/\n",
    "# The docs have been whitespace-condensed\n",
    "# with regex and are now separated by newline\n",
    "# characters.\n",
    "\n",
    "fileObj = open(\"presidocs.txt\", \"r\")\n",
    "fileText = fileObj.read()\n",
    "fileObj.close()\n",
    "\n",
    "docStrings = fileText.split(\"\\n\")\n",
    "print len(docStrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stopwords are exlcluded by default. If we want to include\n",
    "# them, we must add 'stopwords=True' to named arguments.\n",
    "# If we want to stem words, which can reduce dimensionality\n",
    "# for clustering, we add 'stemmer=LEMMA' or 'stemmer=STEMMER'\n",
    "# to named arguments. This makes Document conversion process\n",
    "# take much longer though.\n",
    "\n",
    "docs = [Document(d, stemmer=LEMMA) for d in docStrings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.03846153846153855, u'africa'), (0.021367521367521413, u'african'), (0.01709401709401713, u'agoa'), (0.01709401709401713, u'person'), (0.01709401709401713, u'united'), (0.014957264957264989, u'america'), (0.014957264957264989, u'help'), (0.012820512820512848, u'nation'), (0.012820512820512848, u'thank'), (0.012820512820512848, u'trade')] \n",
      "\n",
      "[(0.06230529595015576, u'nato'), (0.03115264797507788, u'threat'), (0.024922118380062305, u'disarm'), (0.021806853582554516, u'president'), (0.018691588785046728, u'alliance'), (0.018691588785046728, u'freedom'), (0.01557632398753894, u'prague'), (0.01557632398753894, u'q'), (0.012461059190031152, u'21st'), (0.012461059190031152, u'century')] \n",
      "\n",
      "[(0.022684310018903506, u'trade'), (0.018903591682419587, u'china'), (0.018903591682419587, u'issue'), (0.015122873345935671, u'senate'), (0.015122873345935671, u'world'), (0.013232514177693711, u'economy'), (0.013232514177693711, u'future'), (0.013232514177693711, u'person'), (0.013232514177693711, u'senator'), (0.011342155009451753, u'agreement')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print docs[0].keywords(), '\\n'\n",
    "print docs[5].keywords(), '\\n'\n",
    "print docs[1500].keywords(), '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Model(documents=docs, weight=TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0260771401337\n",
      "0.22533937789\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate some cosine similarities\n",
    "print m.similarity(docs[0], docs[1])\n",
    "print m.similarity(docs[5], docs[6])\n",
    "print m.similarity(docs[1000], docs[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kmeans clustering\n",
    "print m.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oh no, that's taking forever.... let's reduce dimensionality...\n",
    "# We'll need to install numpy (pip install numpy) for this to work\n",
    "\n",
    "m.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That also took forever... machine learning can be time consuming\n",
    "# Usually, this stuff is spread across multiple cores.... doing it\n",
    "# on your laptop is less than ideal. But if you're patient, it works!\n",
    "\n",
    "# For the patient, we can use hierarchical clustering, which unlike \n",
    "# kMeans is guaranteed to produce an optimal solution, but it can\n",
    "# take a REALLY long time.... even if we have successfully reduced\n",
    "# the model's dimensionality. (This one originally had 33,000 dimensions!)\n",
    "\n",
    "print m.cluster(method=HIERARCHICAL, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
